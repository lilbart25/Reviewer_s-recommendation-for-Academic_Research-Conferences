{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3Ws0bcs4B3m"
   },
   "source": [
    "### **BERT CODE - FIRST RUN (UNKNOWN TEST AND CSV EMBEDDINGS)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "PGkuvriX4BEY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "aDA5c6zA4dcU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "     ------------------------------------ 236.8/236.8 kB 763.0 kB/s eta 0:00:00\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-1.24.3.tar.gz (10.9 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [40 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\msys64\\mingw64\\lib\\python3.10\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py\", line 351, in <module>\n",
      "      main()\n",
      "    File \"C:\\msys64\\mingw64\\lib\\python3.10\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py\", line 333, in main\n",
      "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "    File \"C:\\msys64\\mingw64\\lib\\python3.10\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py\", line 112, in get_requires_for_build_wheel\n",
      "      backend = _build_backend()\n",
      "    File \"C:\\msys64\\mingw64\\lib\\python3.10\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py\", line 77, in _build_backend\n",
      "      obj = import_module(mod_path)\n",
      "    File \"C:\\msys64\\mingw64\\lib\\python3.10\\importlib\\__init__.py\", line 126, in import_module\n",
      "      return _bootstrap._gcd_import(name[level:], package, level)\n",
      "    File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "    File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "    File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\n",
      "    File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "    File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "    File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "    File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "    File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "    File \"<frozen importlib._bootstrap_external>\", line 887, in exec_module\n",
      "    File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "    File \"C:\\Users\\Shashwat\\AppData\\Local\\Temp\\pip-build-env-ahorufac\\overlay\\lib\\python3.10\\site-packages\\setuptools\\__init__.py\", line 242, in <module>\n",
      "      monkey.patch_all()\n",
      "    File \"C:\\Users\\Shashwat\\AppData\\Local\\Temp\\pip-build-env-ahorufac\\overlay\\lib\\python3.10\\site-packages\\setuptools\\monkey.py\", line 99, in patch_all\n",
      "      patch_for_msvc_specialized_compiler()\n",
      "    File \"C:\\Users\\Shashwat\\AppData\\Local\\Temp\\pip-build-env-ahorufac\\overlay\\lib\\python3.10\\site-packages\\setuptools\\monkey.py\", line 162, in patch_for_msvc_specialized_compiler\n",
      "      patch_func(*msvc9('find_vcvarsall'))\n",
      "    File \"C:\\Users\\Shashwat\\AppData\\Local\\Temp\\pip-build-env-ahorufac\\overlay\\lib\\python3.10\\site-packages\\setuptools\\monkey.py\", line 149, in patch_params\n",
      "      mod = import_module(mod_name)\n",
      "    File \"C:\\msys64\\mingw64\\lib\\python3.10\\importlib\\__init__.py\", line 126, in import_module\n",
      "      return _bootstrap._gcd_import(name[level:], package, level)\n",
      "    File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "    File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "    File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "    File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "    File \"<frozen importlib._bootstrap_external>\", line 887, in exec_module\n",
      "    File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "    File \"C:\\Users\\Shashwat\\AppData\\Local\\Temp\\pip-build-env-ahorufac\\overlay\\lib\\python3.10\\site-packages\\setuptools\\_distutils\\msvc9compiler.py\", line 295, in <module>\n",
      "      raise DistutilsPlatformError(\"VC %0.1f is not supported by this module\" % VERSION)\n",
      "  distutils.errors.DistutilsPlatformError: VC 6.0 is not supported by this module\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: C:\\msys64\\mingw64\\bin\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: C:\\msys64\\mingw64\\bin\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "nlufkUp34ghR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "1tYn7xoK4jwP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shashwat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "KG_Nrkqq4k1v"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import PyPDF2\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "pdf_path = \"C:\\\\Users\\\\Shashwat\\\\Desktop\\\\Shashu\\\\SEM 6\\\\PR 301- Review of research papers\\\\OmPrakash Kaiwartya\\\\A Dynamic Congestion Control Scheme for safety applications.pdf\"\n",
    "csv_path = \"C:\\\\Users\\\\Shashwat\\\\Desktop\\\\Shashu\\\\SEM 6\\\\Project Files\\\\Lemmatized_Output.csv\"\n",
    "text_column = \"PDF\"\n",
    "k = 5\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugXm-Z--47kE"
   },
   "source": [
    "**PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "IecE3rOq4u0P"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    text = re.sub('http[s]?://\\S+', '', text)\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    text = [word for word in text if len(word) > 2]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def preprocess_test_document(pdf_path):\n",
    "    test_document_text = extract_text_from_pdf(pdf_path)\n",
    "    preprocessed_text = preprocess_text(test_document_text)\n",
    "    return preprocessed_text\n",
    "\n",
    "def preprocess_csv_file(csv_path, text_column):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    csv_text_data = data[text_column].tolist()\n",
    "    preprocessed_texts = [preprocess_text(text) for text in csv_text_data]\n",
    "    return preprocessed_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIfFJ2TP5Peq"
   },
   "source": [
    "**TEXT EXTRACTION FOR TEST PDF**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "sG3PdCJL5r5T"
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "            pdf_text = ''\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                pdf_text += page.extract_text()\n",
    "    except KeyError as e:\n",
    "        print(f\"Error processing file '{pdf_path}': {e}\")\n",
    "        return ''\n",
    "    return pdf_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzImP5rM5_4v"
   },
   "source": [
    "**ENCODING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "9BraKjGc6Ioy"
   },
   "outputs": [],
   "source": [
    "def encode_documents(documents):\n",
    "    encoded_texts = tokenizer(documents, padding=True, truncation=True, return_tensors='pt')\n",
    "    return encoded_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcuYANEJ6Mk9"
   },
   "source": [
    "**EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "AQXwohcF6Qkl"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_embeddings(encoded_texts):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        input_ids = encoded_texts['input_ids']\n",
    "        attention_mask = encoded_texts['attention_mask']\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = output.last_hidden_state[:, 0, :]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36OhHATr6UFQ"
   },
   "source": [
    "**SIMILARITY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "6e-7qoEU6YcW"
   },
   "outputs": [],
   "source": [
    "def calculate_similarity(test_embedding, csv_embeddings):\n",
    "    similarity_scores = []\n",
    "    indices = []\n",
    "    for i, csv_embedding in enumerate(csv_embeddings):\n",
    "        similarity = torch.cosine_similarity(test_embedding, csv_embedding)\n",
    "        similarity_scores.append(similarity.item())\n",
    "        indices.append(i)\n",
    "    return csv_embeddings, similarity_scores, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4k_O05H6cRd"
   },
   "source": [
    "**RETRIEVAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "IO-sSYMZ6jby"
   },
   "outputs": [],
   "source": [
    "def retrieve_top_k_unique_authors(csv_data, similarity_scores, indices, k):\n",
    "    sorted_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    top_k_indices = np.array(indices)[top_k_indices]  # Convert indices to NumPy array\n",
    "    top_k_authors = csv_data.iloc[top_k_indices]['AUTHOR'].unique()\n",
    "    return top_k_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "sglOEEVL6mXG"
   },
   "outputs": [],
   "source": [
    "# Batch size for batch processing\n",
    "batch_size = 100\n",
    "\n",
    "# Preprocess the test PDF document\n",
    "preprocessed_test_document = preprocess_test_document(pdf_path)\n",
    "\n",
    "# Preprocess the CSV file\n",
    "preprocessed_csv_texts = preprocess_csv_file(csv_path, text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "zmdkRZq56qb9"
   },
   "outputs": [],
   "source": [
    "# Encode the test document and CSV document text\n",
    "encoded_test_document = encode_documents([preprocessed_test_document])\n",
    "\n",
    "encoded_csv_texts = encode_documents(preprocessed_csv_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "vnYXS0Ux6ziU"
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for the test document\n",
    "test_embedding = generate_embeddings(encoded_test_document) \n",
    "\n",
    "# Save the test embedding to a file\n",
    "np.save(\"test_embedding.npy\", test_embedding.numpy())\n",
    "\n",
    "# Load the test embedding from the file\n",
    "test_embedding = torch.from_numpy(np.load(\"test_embedding.npy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jm5NjK1362-r"
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for the CSV file in batches\n",
    "csv_embeddings = []\n",
    "num_batches = int(np.ceil(len(encoded_csv_texts.input_ids) / batch_size))\n",
    "for i in range(num_batches):\n",
    "    batch_inputs = {\n",
    "        'input_ids': encoded_csv_texts.input_ids[i * batch_size:(i + 1) * batch_size],\n",
    "        'attention_mask': encoded_csv_texts.attention_mask[i * batch_size:(i + 1) * batch_size]\n",
    "    }\n",
    "    batch_embeddings = generate_embeddings(batch_inputs)\n",
    "    csv_embeddings.append(batch_embeddings)\n",
    "\n",
    "# Concatenate the embeddings from different batches\n",
    "csv_embeddings = torch.cat(csv_embeddings)\n",
    "\n",
    "# Save the CSV embeddings to a file\n",
    "np.save(\"csv_embeddings2.npy\", csv_embeddings.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xp0_ZIu468dG"
   },
   "outputs": [],
   "source": [
    "# Calculate similarity between the test document and CSV documents\n",
    "csv_embeddings, similarity_scores, indices = calculate_similarity(test_embedding, csv_embeddings)\n",
    "\n",
    "# Load the CSV data\n",
    "csv_data = pd.read_csv(csv_path)\n",
    "\n",
    "# Retrieve top-k unique authors\n",
    "top_k_authors = retrieve_top_k_unique_authors(csv_data, similarity_scores, indices, k)\n",
    "\n",
    "# Print the top-k authors\n",
    "print(top_k_authors.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQQiBEYz8LFm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttxJcdvP7BKm"
   },
   "source": [
    "# **BERT CODE -(NEW TEST DOC AND EXISTING CSV EMBEDDINGS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "gCJ9x3xl63CB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import PyPDF2\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "pdf_path = \"C:\\\\Users\\\\Shashwat\\\\Desktop\\\\Shashu\\\\SEM 6\\\\PR 301- Review of research papers\\\\OmPrakash Kaiwartya\\\\A Dynamic Congestion Control Scheme for safety applications.pdf\"\n",
    "csv_path = \"C:\\\\Users\\\\Shashwat\\\\Desktop\\\\Shashu\\\\SEM 6\\\\Project Files\\\\Lemmatized_Output.csv\"\n",
    "text_column = \"PDF\"\n",
    "k = 10\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4pg7rai7m8A"
   },
   "source": [
    " **FUNCTION DEFINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "0_4pd0Cz7dp1"
   },
   "outputs": [],
   "source": [
    "def preprocess_test_document(pdf_path):\n",
    "    test_document_text = extract_text_from_pdf(pdf_path)\n",
    "    preprocessed_text = preprocess_text(test_document_text)\n",
    "    return preprocessed_text\n",
    "\n",
    "def preprocess_csv_file(csv_path, text_column):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    csv_text_data = data[text_column].tolist()\n",
    "    preprocessed_texts = [preprocess_text(text) for text in csv_text_data]\n",
    "    return preprocessed_texts\n",
    "\n",
    "def encode_documents(documents):\n",
    "    encoded_texts = tokenizer(documents, padding=True, truncation=True, return_tensors='pt')\n",
    "    return encoded_texts\n",
    "\n",
    "def generate_embeddings(encoded_texts):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        input_ids = encoded_texts['input_ids']\n",
    "        attention_mask = encoded_texts['attention_mask']\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = output.last_hidden_state[:, 0, :]\n",
    "    return embeddings\n",
    "\n",
    "def calculate_similarity(test_embedding, csv_embeddings):\n",
    "    similarity_scores = []\n",
    "    indices = []\n",
    "    for i, csv_embedding in enumerate(csv_embeddings):\n",
    "        similarity = torch.cosine_similarity(test_embedding, csv_embedding)\n",
    "        similarity_scores.append(similarity.item())\n",
    "        indices.append(i)\n",
    "    return csv_embeddings, similarity_scores, indices\n",
    "\n",
    "def retrieve_top_k_unique_authors(csv_data, similarity_scores, indices, k):\n",
    "    sorted_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    top_k_indices = np.array(indices)[top_k_indices]  # Convert indices to NumPy array\n",
    "    top_k_authors = csv_data.iloc[top_k_indices]['AUTHOR'].unique()\n",
    "    return top_k_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "9DWUrBAs70bc"
   },
   "outputs": [],
   "source": [
    "# Preprocess the test PDF document\n",
    "preprocessed_test_document = preprocess_test_document(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "JOYIg5WX71Ww"
   },
   "outputs": [],
   "source": [
    "# Encode the test document and CSV document text\n",
    "encoded_test_document = encode_documents([preprocessed_test_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "uWiL5wKY75hi"
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for the test document\n",
    "test_embedding = generate_embeddings(encoded_test_document) \n",
    "\n",
    "# Save the test embedding to a file\n",
    "np.save(\"test_embedding.npy\", test_embedding.numpy())\n",
    "\n",
    "# Load the test embedding from the file\n",
    "test_embedding = torch.from_numpy(np.load(\"test_embedding.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "_8ok7QT476eg"
   },
   "outputs": [],
   "source": [
    "csv_embeddings = torch.from_numpy(np.load(\"csv_embeddings_final.npy\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "wPVpehAz7-rA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tandra Pal', 'Dr.Rohit Beniwal', 'Barsha Mitra', 'Prof. B Subudhi', 'Tingwen Huang', 'Aruna Malapati', 'Dilip Singh Sisodia', 'Amit Saxena']\n"
     ]
    }
   ],
   "source": [
    "def retrieve_top_k_unique_authors(csv_data, similarity_scores, indices, k):\n",
    "    sorted_indices = np.argsort(similarity_scores)[::-1]  # Sort indices in descending order\n",
    "    valid_indices = [idx for idx in sorted_indices if idx < len(csv_data)]  # Filter out-of-bounds indices\n",
    "    top_k_indices = valid_indices[:k]  # Select top-k valid indices\n",
    "    top_k_authors = csv_data.iloc[top_k_indices]['AUTHOR'].unique()\n",
    "    return top_k_authors\n",
    "\n",
    "# Calculate similarity between the test document and CSV documents\n",
    "csv_embeddings, similarity_scores, indices = calculate_similarity(test_embedding, csv_embeddings)\n",
    "\n",
    "# Load the CSV data\n",
    "csv_data = pd.read_csv(csv_path)\n",
    "\n",
    "# Retrieve top-k unique authors\n",
    "top_k_authors = retrieve_top_k_unique_authors(csv_data, similarity_scores, indices, k)\n",
    "\n",
    "# Print the top-k authors\n",
    "print(top_k_authors.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
